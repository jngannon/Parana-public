{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment progress\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential articles\n",
    "\n",
    "Going to start structuring work around what i need to write these articles. This bit is plans, what i need to build for the package, ramblings go elsewhere. \n",
    "\n",
    "## Pruning for minimal size\n",
    "Started, use that notebook now.\n",
    "\n",
    "\n",
    "## Extra parameters help training\n",
    "This one is a theory, most of the software framework is there, will reference the first.\n",
    "models pruned down to a size perform better than networks trained at that size. Proof is a plot.\n",
    "is this because of initialization, more parameters to choose from chances are some of them will be better?\n",
    "Test that by saving initialization values, loading from them and training again.\n",
    "Probably best explored with noisy wiggles every x training steps. \n",
    "Will be a lot of information to deal with, maybe a histogram of how many parameters within this range of wigglyness\n",
    "\n",
    "Should see if i can find some parameters that go from having low to high and then back to low values again. thinking about having a look at the output of the multiplication before summation.\n",
    "\n",
    "### sort and count:\n",
    "sum(value>array>other_value) for range of values, will have to play with bin sizes\n",
    "\n",
    "## Compression of parameters\n",
    "Not even sure if there is enough here, as above, will need to be looking for evidence and building an article around it\n",
    "\n",
    "## Parameter visualisation\n",
    "Only have some thoughts so far, have both software and theory behind it to build yet\n",
    "\n",
    "## Pruning overlapping parameters\n",
    "ONce parameter visualisations are working a bit, this should be easy enough to implement, the aricle itself depends on what evidence i find. Potentially improve accuracy or certainty by altering parameters that overlap between classes.\n",
    "\n",
    "## Pruning overlapping parameters with adversarial noise\n",
    "Depends on above, but the theory is the same, adversarial noise might be triggering a few parameters that can be removed while keeping functionallity of the model. This could potentially be very useful.\n",
    "### Add adversarial noise:\n",
    "will need to calculate some noise, maybe save some pickle files to load\n",
    "\n",
    "## CNN parameter analysis\n",
    "Will need to get cnn layers working in my model first, and get a few results, want this working so that i can put out stuff in a short period of time\n",
    "\n",
    "## Bigger than toy networks\n",
    "As above, just something to say that there is something more than fringe results on toy models\n",
    "\n",
    "## Transfer learning/Adding tasks\n",
    "Do some experiments on VGG CNNs, maybe a built in layer function that loads the weights, and trainable layers on top.\n",
    "I have some ideas about analysing parameters, i will need to consider how to train them, do i only apply gradients to a selection of parameters? Do i run the forward pass and take gradients wrt a selection of parameters?\n",
    "\n",
    "Could consider different types of filters rather than a binary filter, a learning rate filter that trains some parameters by a large ammount and others by a small ammount. \n",
    "\n",
    "Arun Mallya and Svetlana Lazebnik, paper on adding tasks to a fixed network. Use absolute magnitude work out which parameters to prune from a network, while taking 30% of the network drawn from the bottom 50% seems well within any threshold on my toy networks, better selection could lead to improvements. \n",
    "\n",
    "I will need to work with conv nets quite a lot first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization\n",
    "\n",
    "I had some interesting hints from earlier experiments that got the generalization error down at certain points by pruning the highest absolute deviation parameters. i had the thought of gradually adding noise to the input vectors of training and test sets and seeing when the accuracy started to fall off as a more noise is added. \n",
    "\n",
    "addtopackage\n",
    "\n",
    "1. label randomizer\n",
    "2. progressive noise adder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter analysis\n",
    "\n",
    "I have a couple of things to think about, wrt input noise. \n",
    "\n",
    "Then i need to consider a way to compare parameter rankings, mean absolute distance seems a good start. First as a proof of concept i need to find parameters with small weight values that have a big influence on outputs, and big weight values that have a small influence on outputs. \n",
    "\n",
    "## absolute distance metric \n",
    "Easy enough to implement\n",
    "\n",
    "## outliers\n",
    "Going to take some experimentation to find out:\n",
    "\n",
    "1. How many to look for.\n",
    "2. How to evaluate them\n",
    "3. If they are useful for what tasks\n",
    "\n",
    "Take x ammount, apply a mask, test. \n",
    "Can i drop the accuracy for 1 class?\n",
    "Can i find any parameters that are significant for 2 similar classes but not others, ex. 2 and 3 but not 1?\n",
    "In earlier experiments i found that i could completely kill the accuracy by reinitializing ~1% of parameters, i should try to find the smallest ammount I can to get a perceptable change in performance. \n",
    "\n",
    "## Individual class accuracy \n",
    "tf.metrics.mean_per_class_accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
