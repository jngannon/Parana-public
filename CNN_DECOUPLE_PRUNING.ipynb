{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model\n",
    "\n",
    "This cell uses model, layer and optimizer modules. The model class has a couple of interesting things to mention, There are 2 cost functions. Cost function calculates a cost with softmax applied to the final layer, there are also cross entropy with and without L2 regularization. Logit cost function calculates the cost without applying a softmax function. Cross entropy will not work without a softmax function (negative values are not defined), so there are only quadratic cost functions. \n",
    "\n",
    "Layers are added to the model by appending to the layers variable. Weight decay and bias decay are the L2 regularization constants. Setting weight init to xavier will use Xavier initialization, setting a small constant will use the constant as the standard deviation of a truncated normal distribution, and setting a numpy array will initialize the weights using the array. \n",
    "\n",
    "Adamopt here is the tensorflow implementation of Adam optimizer, the wrapper module is just for a standard API because i made another optimizer for a different experiment. Using the tensorflow one would work exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jim\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from parana.Model import Model\n",
    "from parana.Layers import fc_layer\n",
    "from parana.Layers import softmax_layer\n",
    "from parana.optimizer import optimizer\n",
    "from parana.optimizer import adamopt\n",
    "from parana.Layers import conv_layer\n",
    "from parana.parameter_saver import saver\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True, reshape=False)\n",
    "from IPython.display import clear_output\n",
    "from parana.parameter_pruning import sparse_lobotomizer\n",
    "from parana.sparse_selection import get_k_min\n",
    "from parana.sparse_wiggles import get_mean_activations\n",
    "from parana.sparse_wiggles import get_abs_values\n",
    "path = 'c:/users/jim/tensorflowtrials/'\n",
    "import pickle\n",
    "\n",
    "class this_model(Model):\n",
    "    \n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        self.cost_function = 'cross_entropy_l2'\n",
    "        self.logit_cost_function = 'quadratic_l2'\n",
    "        self.dropout = 0.7\n",
    "        self.layers = [conv_layer(inputs = inputs,\n",
    "                                 height = 7, \n",
    "                                 width = 7, \n",
    "                                 filters = 12, \n",
    "                                 padding = 4, \n",
    "                                 stride = 1,\n",
    "                                 flatten = False,\n",
    "                                 weight_init = 'xavier',\n",
    "                                 weight_decay=0.0001, bias_decay=0.0001)]\n",
    "        self.layers.append(conv_layer(inputs = self.layers[0].activate,\n",
    "                                 height = 5, \n",
    "                                 width = 5, \n",
    "                                 filters = 15, \n",
    "                                 padding = 2, \n",
    "                                 stride = 1,\n",
    "                                 flatten = False,\n",
    "                                 weight_init = 'xavier',\n",
    "                                 weight_decay=0.0001, bias_decay=0.0001))\n",
    "        self.layers.append(conv_layer(inputs = self.layers[1].activate,\n",
    "                                 height = 3, \n",
    "                                 width = 3, \n",
    "                                 filters = 25, \n",
    "                                 padding = 1, \n",
    "                                 stride = 1,\n",
    "                                 flatten = False,\n",
    "                                 weight_init = 'xavier',\n",
    "                                 weight_decay=0.0001, bias_decay=0.0001))\n",
    "        self.layers.append(conv_layer(inputs = self.layers[2].activate,\n",
    "                                 height = 3, \n",
    "                                 width = 3, \n",
    "                                 filters = 20, \n",
    "                                 padding = 1, \n",
    "                                 stride = 1,\n",
    "                                 flatten = True,\n",
    "                                 weight_init = 'xavier',\n",
    "                                 weight_decay=0.0001, bias_decay=0.0001))\n",
    "        self.layers.append(fc_layer(inputs = self.layers[3].activate,\n",
    "                                   size = 1500,\n",
    "                                   weight_init = 'xavier',\n",
    "                                   weight_decay=0.0001, bias_decay=0.0001))\n",
    "        self.layers.append(fc_layer(inputs=self.layers[4].activate, \n",
    "                               weight_init = 'xavier',\n",
    "                               size=500, \n",
    "                               weight_decay=0.0001, bias_decay=0.0001))\n",
    "        self.layers.append(softmax_layer(inputs=self.layers[5].activate, \n",
    "                                          size=10, \n",
    "                                          weight_decay=0.0001, bias_decay=0.0001))\n",
    "\n",
    "X = tf.placeholder('float', [None, 28, 28, 1], name = 'Inputs')\n",
    "y = tf.placeholder('float', [None, 10], name = 'Labels')   \n",
    "\n",
    "noise = tf.Variable(tf.zeros([28,28,1]), name='x_noise')\n",
    "set_zero_noise = tf.assign(noise, tf.zeros([28,28,1]))\n",
    "noise_placeholder = tf.placeholder('float', [28, 28,1], name = 'noise_placeholder')\n",
    "assign_noise = tf.assign(noise, noise_placeholder)\n",
    "X_noise = X + noise\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "mymodel = this_model(X_noise, y)\n",
    "\n",
    "opt = adamopt(session = sess,\n",
    "              learning_rate = 0.0001,\n",
    "              cost_function = mymodel.cost, \n",
    "              model =  mymodel)\n",
    "opt2 = adamopt(session = sess,\n",
    "              learning_rate = 0.000005,\n",
    "              cost_function = mymodel.cost, \n",
    "              model =  mymodel)\n",
    "model_saver = saver(mymodel, sess)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the weights\n",
    "Instead of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters loaded from  c:/users/jim/tensorflowtrials/cnn99.34%.p\n",
      "0.9934869766235351\n"
     ]
    }
   ],
   "source": [
    "path = 'c:/users/jim/tensorflowtrials/'\n",
    "model_saver.load_parameters('{}cnn99.34%.p'.format(path))\n",
    "print(model_saver.split_accuracy(session = sess,\n",
    "                       stages=20,\n",
    "                       inputs = mnist.test.images, \n",
    "                       labels = mnist.test.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decouple weights and sparsemodel\n",
    "\n",
    "Sparse decouple weights unrolls each convolutional filter into the single row of a 2d matrix. These are stored as scipy csr matrices. It also assigns biases to each activation in a numpy array.\n",
    "\n",
    "Sparsemodel defines a new model that uses python operations. Tensorflow does not support sparse operations, and the full arrays will not fit in memory. This will run much slower than on tensorflow, but performance improves when parameters are pruned from the model.\n",
    "\n",
    "The input will now be a flat array, so i have loaded the tensorflow mnist loader again flattening the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Close your tensorflow session, it is no longer useful\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True, reshape=True)\n",
    "mymodel.sparsemodel(mymodel.sparse_decouple_weights(sess))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning by activation values here\n",
    "\n",
    "The sparse_lobotomizer module is named because it removes weights (or connections) from the model, bad joke but the name stuck. There are 2 main functions that this performs, get_wigglyness gets a value for each parameter of the network and prune_step prunes a ratio of weights based on these values.\n",
    "\n",
    "Get_mean_activations multiplies each weight value by its corresponding input for each data point(100 here), and takes the mean.\n",
    "\n",
    "Get_k_min just gets the indices of the minimum valued parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<784x9408 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 348684 stored elements in Compressed Sparse Row format>,\n",
       " <9408x11760 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 3232080 stored elements in Compressed Sparse Row format>,\n",
       " <11760x19600 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 2521500 stored elements in Compressed Sparse Row format>,\n",
       " <19600x15680 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 3361920 stored elements in Compressed Sparse Row format>,\n",
       " <15680x1500 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 23073000 stored elements in Compressed Sparse Row format>,\n",
       " <1500x500 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 263000 stored elements in Compressed Sparse Row format>,\n",
       " <500x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 4840 stored elements in Compressed Sparse Row format>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'c:/users/jim/tensorflowtrials/'\n",
    "activation_screwdriver = sparse_lobotomizer(model = mymodel,\n",
    "                                 layers_list = mymodel.layers,\n",
    "                                 wigglyness = get_mean_activations,\n",
    "                                 iterations = 100,\n",
    "                                 parameter_selection = get_k_min,\n",
    "                                 data_function = mnist.train.next_batch)\n",
    "#activation_screwdriver._arrays = pickle.load(open('{}activation_screwdriver.p'.format(path), 'rb'))\n",
    "print('loaded')\n",
    "activation_screwdriver.get_wigglyness(iterations = 1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the parameter data\n",
    "These models take a long time to work with, so I have saved the mean activation values for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(activation_screwdriver._arrays, open('{}activation_screwdriver99.34%.p'.format(path), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gotit\n"
     ]
    }
   ],
   "source": [
    "abs_values = sparse_lobotomizer(model = mymodel,\n",
    "                                 layers_list = mymodel.layers,\n",
    "                                 wigglyness = get_abs_values,\n",
    "                                 iterations = 100,\n",
    "                                 parameter_selection = get_k_min,\n",
    "                                 data_function = mnist.train.next_batch)\n",
    "abs_values.get_wigglyness()\n",
    "print('gotit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune step\n",
    "\n",
    "I have split the pruning for this model into 3 sections, convolution layers, fully connected layers, and softmax layer. Even though i am not applying a softmax function to the final layer, I am working on the assumption that being the final layer, and smaller than the others, it will behave differently to pruning. Models could be split further, or even pruned on an individual layer level, but this is very time consuming, especially when not run on a GPU. \n",
    "\n",
    "I set up a single script and run it several times with different parameters. The run time on this is directly proportional to the number of parameters, so to prune efficiently start with big steps then move on to a finer grain. Most experiments on this model and others that i have tested removing 90% of parameters has no real effect on performance, this means a 10X speed up for sparse_model. Things are going to scale differently for tensorflow or other GPU optimized frameworks. \n",
    "\n",
    "Checking accuracy with the full test set of 10,000 images is slow, so i test to start with on 1,000 after convolutional layers have been heavily pruned, things speed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_saver.store_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_saver.restore_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.984\n",
      "that took 9.984939575195312\n",
      "0.226\n",
      "that took 19.959836959838867\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pickle\n",
    "path = 'c:/users/jim/tensorflowtrials/cnn_adv_noise/'\n",
    "noisevector = pickle.load(open('{}adv_class_5_limit_0.3.p'.format(path), 'rb'))\n",
    "noisevector = np.reshape(noisevector, -1)\n",
    "inputs = mnist.test.images[:1000]\n",
    "noisy_inputs = inputs + noisevector\n",
    "labels = mnist.test.labels[:1000]\n",
    "tic = time.time()\n",
    "print (mymodel.sparse_accuracy(inputs, labels))\n",
    "print('that took', time.time() - tic)\n",
    "print (mymodel.sparse_accuracy(noisy_inputs, labels))\n",
    "print('that took', time.time() - tic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRunEd\n"
     ]
    }
   ],
   "source": [
    "activation_screwdriver.prune_step(0.5, layers_list = mymodel.layers[:3])\n",
    "print('PRunEd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRunEd\n"
     ]
    }
   ],
   "source": [
    "activation_screwdriver.prune_step(0.985, layers_list = [mymodel.layers[4]])\n",
    "print('PRunEd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prUNeD\n"
     ]
    }
   ],
   "source": [
    "abs_values.prune_step(0.9, layers_list = [mymodel.layers[4]])\n",
    "print('prUNeD')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
